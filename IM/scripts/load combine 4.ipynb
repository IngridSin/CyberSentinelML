{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-03T19:34:15.142599Z",
     "start_time": "2025-04-03T19:34:06.393503Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import hstack\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Text Cleaning Function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "# Load the bundled pipeline\n",
    "pipeline = joblib.load(r\"C:\\Users\\may\\Desktop\\phishing_pipeline.joblib\")\n",
    "ensemble_model = pipeline['ensemble_model']\n",
    "tfidf_subject = pipeline['tfidf_subject']\n",
    "tfidf_body = pipeline['tfidf_body']\n",
    "\n",
    "# Access fitted base models from the ensemble\n",
    "models = ensemble_model.named_estimators_\n",
    "\n",
    "# Function to process input and return predictions with top 5 words from all models\n",
    "def predict_phishing(id, subject, body):\n",
    "    # Clean input\n",
    "    cleaned_subject = clean_text(subject)\n",
    "    cleaned_body = clean_text(body)\n",
    "\n",
    "    # Vectorize using loaded TF-IDF vectorizers\n",
    "    X_subject = tfidf_subject.transform([cleaned_subject])\n",
    "    X_body = tfidf_body.transform([cleaned_body])\n",
    "    X_combined = hstack((X_subject, X_body))\n",
    "\n",
    "    # Ensemble prediction\n",
    "    prediction = ensemble_model.predict(X_combined)[0]\n",
    "    ensemble_probs = ensemble_model.predict_proba(X_combined)[0]\n",
    "    ensemble_prob_class0 = ensemble_probs[0]\n",
    "    ensemble_prob_class1 = ensemble_probs[1]\n",
    "\n",
    "    # Individual model predictions and probabilities\n",
    "    model_scores = {}\n",
    "    for name, model in models.items():\n",
    "        pred = model.predict(X_combined)[0]\n",
    "        probs = model.predict_proba(X_combined)[0]\n",
    "        prob_class_0 = probs[0]\n",
    "        prob_class_1 = probs[1]\n",
    "\n",
    "        model_scores[f\"{name.lower()}_pred\"] = pred\n",
    "        model_scores[f\"{name.lower()}_prob_class0\"] = prob_class_0\n",
    "        model_scores[f\"{name.lower()}_prob_class1\"] = prob_class_1\n",
    "\n",
    "    # Determine winner\n",
    "    winner_key = max(\n",
    "        model_scores,\n",
    "        key=lambda k: model_scores[f\"{k.split('_')[0]}_prob_class1\"] if model_scores[k] == prediction else -1\n",
    "    )\n",
    "    winner = winner_key.split('_')[0]\n",
    "    winner_probability = (\n",
    "        model_scores[f\"{winner}_prob_class1\"] if prediction == 1\n",
    "        else model_scores[f\"{winner}_prob_class0\"]\n",
    "    )\n",
    "\n",
    "    # Get feature names\n",
    "    subject_features = tfidf_subject.get_feature_names_out()\n",
    "    body_features = tfidf_body.get_feature_names_out()\n",
    "    all_features = np.concatenate([subject_features, body_features])\n",
    "\n",
    "    # Get feature contributions for all models\n",
    "    X_combined_dense = X_combined.toarray()[0]\n",
    "    all_model_contributions = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        feature_contributions = {}\n",
    "\n",
    "        if hasattr(model, 'feature_importances_'):  # Random Forest\n",
    "            importances = model.feature_importances_\n",
    "            for i, (feature, importance) in enumerate(zip(all_features, importances)):\n",
    "                if X_combined_dense[i] > 0:\n",
    "                    feature_contributions[feature] = importance * X_combined_dense[i]\n",
    "        elif hasattr(model, 'coef_'):  # Linear SVM\n",
    "            coef = model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_\n",
    "            for i, (feature, coef_val) in enumerate(zip(all_features, coef)):\n",
    "                if X_combined_dense[i] > 0:\n",
    "                    feature_contributions[feature] = coef_val * X_combined_dense[i]\n",
    "        elif hasattr(model, 'feature_log_prob_'):  # Naive Bayes\n",
    "            log_probs = model.feature_log_prob_[1]  # Phishing class\n",
    "            for i, (feature, log_prob) in enumerate(zip(all_features, log_probs)):\n",
    "                if X_combined_dense[i] > 0:\n",
    "                    feature_contributions[feature] = log_prob * X_combined_dense[i]\n",
    "\n",
    "        # Get top 5 features for this model\n",
    "        top_features = sorted(feature_contributions.items(), key=lambda x: abs(x[1]), reverse=True)[:5]\n",
    "        all_model_contributions[f\"top_5_words_from_{name.lower()}\"] = {feature: float(score) for feature, score in top_features}\n",
    "\n",
    "    # Build result dictionary\n",
    "    result = {\n",
    "        \"id\": id,\n",
    "        \"subject\": subject,\n",
    "        \"body\": body,\n",
    "        \"prediction\": int(prediction),\n",
    "        \"winner_model\": winner,\n",
    "        \"winner_probability\": float(winner_probability),\n",
    "        \"ensemble_predicted_label\": int(prediction),\n",
    "        \"ensemble_prob_class0\": float(ensemble_prob_class0),\n",
    "        \"ensemble_prob_class1\": float(ensemble_prob_class1),\n",
    "        **{f\"{name.lower()}_pred\": int(model_scores[f\"{name.lower()}_pred\"])\n",
    "           for name in models.keys()},\n",
    "        **{f\"{name.lower()}_prob_class0\": float(model_scores[f\"{name.lower()}_prob_class0\"])\n",
    "           for name in models.keys()},\n",
    "        **{f\"{name.lower()}_prob_class1\": float(model_scores[f\"{name.lower()}_prob_class1\"])\n",
    "           for name in models.keys()},\n",
    "        **all_model_contributions  # Add top 5 words from all models\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    # Test case 1: A potentially phishing email\n",
    "    test_id_1 = \"001\"\n",
    "    test_subject_1 = \"Urgent: Verify Your Account Now!\"\n",
    "    test_body_1 = \"Dear user, your account will be suspended unless you click this link to verify: http://fake-login.com. Act now!\"\n",
    "\n",
    "    result_1 = predict_phishing(test_id_1, test_subject_1, test_body_1)\n",
    "    print(\"Test Case 1 (Phishing Example):\")\n",
    "    print(json.dumps(result_1, indent=4))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1 (Phishing Example):\n",
      "{\n",
      "    \"id\": \"001\",\n",
      "    \"subject\": \"Urgent: Verify Your Account Now!\",\n",
      "    \"body\": \"Dear user, your account will be suspended unless you click this link to verify: http://fake-login.com. Act now!\",\n",
      "    \"prediction\": 1,\n",
      "    \"winner_model\": \"xgb\",\n",
      "    \"winner_probability\": 0.9993987083435059,\n",
      "    \"ensemble_predicted_label\": 1,\n",
      "    \"ensemble_prob_class0\": 0.0622091823505867,\n",
      "    \"ensemble_prob_class1\": 0.9377908176494133,\n",
      "    \"nb_pred\": 1,\n",
      "    \"rf_pred\": 1,\n",
      "    \"xgb_pred\": 1,\n",
      "    \"knn_pred\": 1,\n",
      "    \"logreg_pred\": 1,\n",
      "    \"nb_prob_class0\": 0.0023386330892434855,\n",
      "    \"rf_prob_class0\": 0.14029850746268657,\n",
      "    \"xgb_prob_class0\": 0.0006012916564941406,\n",
      "    \"knn_prob_class0\": 0.16666666666666666,\n",
      "    \"logreg_prob_class0\": 0.001140812877842623,\n",
      "    \"nb_prob_class1\": 0.9976613669107564,\n",
      "    \"rf_prob_class1\": 0.8597014925373134,\n",
      "    \"xgb_prob_class1\": 0.9993987083435059,\n",
      "    \"knn_prob_class1\": 0.8333333333333334,\n",
      "    \"logreg_prob_class1\": 0.9988591871221574,\n",
      "    \"top_5_words_from_nb\": {\n",
      "        \"now\": -4.597176081124212,\n",
      "        \"urgent\": -4.46455662561214,\n",
      "        \"login\": -3.2675152226142115,\n",
      "        \"suspended\": -3.086423084022856,\n",
      "        \"act\": -2.827118759404564\n",
      "    },\n",
      "    \"top_5_words_from_rf\": {\n",
      "        \"com\": 0.0009887081589948098,\n",
      "        \"http\": 0.0006411290135969516,\n",
      "        \"click\": 0.0005937928328180041,\n",
      "        \"account\": 0.0004226210774593808,\n",
      "        \"dear\": 0.00042111076197516795\n",
      "    },\n",
      "    \"top_5_words_from_xgb\": {\n",
      "        \"click\": 0.0008387634527379478,\n",
      "        \"account\": 0.0008004064010790848,\n",
      "        \"com\": 0.00034805660565077033,\n",
      "        \"dear\": 0.00029738309694746673,\n",
      "        \"link\": 0.00011695873221752041\n",
      "    },\n",
      "    \"top_5_words_from_knn\": {},\n",
      "    \"top_5_words_from_logreg\": {\n",
      "        \"account\": 1.5791620412772478,\n",
      "        \"click\": 1.411884910559085,\n",
      "        \"act\": 1.0905116590118331,\n",
      "        \"dear\": 1.048579096161762,\n",
      "        \"suspended\": 1.0191270980886296\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
